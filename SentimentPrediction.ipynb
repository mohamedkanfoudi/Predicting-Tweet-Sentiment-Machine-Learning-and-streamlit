{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b11fd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# ML Libraries\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e4fd62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Parameters\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# 0 negative\n",
    "# 2 neutral\n",
    "# 4 positive\n",
    "def load_dataset(filename, cols):\n",
    "    dataset = pd.read_csv(filename, encoding='latin-1')\n",
    "    dataset.columns = cols\n",
    "    return dataset\n",
    "\n",
    "def remove_unwanted_cols(dataset, cols):\n",
    "    for col in cols:\n",
    "        del dataset[col]\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def preprocess_tweet_text(tweet):\n",
    "    tweet.lower()\n",
    "    # Remove urls\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet, flags=re.MULTILINE)\n",
    "    # Remove user @ references and '#' from tweet\n",
    "    tweet = re.sub(r'\\@\\w+|\\#','', tweet)\n",
    "    # Remove punctuations\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove stopwords\n",
    "    tweet_tokens = word_tokenize(tweet)\n",
    "    filtered_words = [w for w in tweet_tokens if not w in stop_words]\n",
    "    \n",
    "    #ps = PorterStemmer()\n",
    "    #stemmed_words = [ps.stem(w) for w in filtered_words]\n",
    "    #lemmatizer = WordNetLemmatizer()\n",
    "    #lemma_words = [lemmatizer.lemmatize(w, pos='a') for w in stemmed_words]\n",
    "    \n",
    "    return \" \".join(filtered_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1edc077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_vector(train_fit):\n",
    "    vector = TfidfVectorizer(sublinear_tf=True)\n",
    "    vector.fit(train_fit)\n",
    "    return vector\n",
    "\n",
    "def int_to_string(sentiment):\n",
    "    if sentiment == 0:\n",
    "        return \"Negative\"\n",
    "    elif sentiment == 2:\n",
    "        return \"Neutral\"\n",
    "    else:\n",
    "        return \"Positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a94c801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset.head()\n",
      "   target        t_id                    created_at     query           user  \\\n",
      "0       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY  scotthamilton   \n",
      "1       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY       mattycus   \n",
      "2       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY        ElleCTF   \n",
      "3       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY         Karoli   \n",
      "4       0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY       joy_wolf   \n",
      "\n",
      "                                                text  \n",
      "0  is upset that he can't update his Facebook by ...  \n",
      "1  @Kenichan I dived many times for the ball. Man...  \n",
      "2    my whole body feels itchy and like its on fire   \n",
      "3  @nationwideclass no, it's not behaving at all....  \n",
      "4                      @Kwesidei not the whole crew   \n"
     ]
    }
   ],
   "source": [
    "# paste your link to load the training_train-1M6.csv"
    "dataset = load_dataset("C:/Users/HPr/Desktop/2GI/ml/Predicting-Tweet-Sentiment-Maching-Learning-and-streamlit/training_train-1M6.csv", ['target', 't_id', 'created_at', 'query', 'user', 'text'])
",
    "\n",
    "print(\"dataset.head()\")\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e754e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unwanted columns from dataset\n",
    "n_dataset = remove_unwanted_cols(dataset, ['t_id', 'created_at', 'query', 'user'])\n",
    "#Preprocess data\n",
    "dataset.text = dataset['text'].apply(preprocess_tweet_text)\n",
    "\n",
    "# Same tf vector will be used for Testing sentiments on unseen trending data\n",
    "tf_vector = get_feature_vector(np.array(dataset.iloc[:, 1]).ravel())\n",
    "X = tf_vector.transform(np.array(dataset.iloc[:, 1]).ravel())\n",
    "y = np.array(dataset.iloc[:, 0]).ravel()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e93f0735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(sublinear_tf=True)\n"
     ]
    }
   ],
   "source": [
    "print(tf_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7653efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes model  :  accuracy_score :  0.76797563431459\n"
     ]
    }
   ],
   "source": [
    "# Training Naive Bayes model\n",
    "NB_model = MultinomialNB()\n",
    "NB_model.fit(X_train, y_train)\n",
    "y_predict_nb = NB_model.predict(X_test)\n",
    "print(\"Naive Bayes model  :  accuracy_score : \",accuracy_score(y_test, y_predict_nb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91296a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistics Regression model : accuracy_score :  0.7879478664390202\n"
     ]
    }
   ],
   "source": [
    "# Training Logistics Regression model\n",
    "LR_model = LogisticRegression(solver='lbfgs')\n",
    "LR_model.fit(X_train, y_train)\n",
    "y_predict_lr = LR_model.predict(X_test)\n",
    "print(\"Logistics Regression model : accuracy_score : \",accuracy_score(y_test, y_predict_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c0a8af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1203e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6fc64774",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"model\": LR_model ,\"tf_vector\" : tf_vector}\n",
    "with open('saved_steps1.pkl', 'wb') as file:\n",
    "    pickle.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6aefe42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_steps1.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "regressor_loaded = data[\"model\"]\n",
    "vectorTF = data[\"tf_vector\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d8a76fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_ds.head()\n",
      "      hashtag                                               text\n",
      "0  2326531889             getting ready to go to work its sucks \n",
      "1  2326531983  O good lord, I have to learn how to use Castor...\n",
      "2  2326532300              my arms are sore from swim practice. \n",
      "3  2326532369  is craving a pack of crisps but im on a diet  lol\n",
      "4  2326532651                   can't send sms in gmail anymore \n"
     ]
    }
   ],
   "source": [
    "test_file_name = \"C:/Users/HPr/Desktop/Stage/twitter_python/training_test_1000.csv\"\n",
    "test_ds = load_dataset(test_file_name, ['target', 'hashtag', 'created_at', 'query', 'user', 'text'])\n",
    "test_ds = remove_unwanted_cols(test_ds, [\"target\", \"created_at\",\"query\", \"user\"])\n",
    "\n",
    "print(\"test_ds.head()\")\n",
    "print(test_ds.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df33e3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating text feature\n",
    "test_ds.text = test_ds[\"text\"].apply(preprocess_tweet_text)\n",
    "test_feature = vectorTF.transform(np.array(test_ds.iloc[:, 1]).ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "73bc21f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Logistic Regression model for prediction\n",
    "\n",
    "#test_prediction_lr = LR_model.predict(test_feature)\n",
    "\n",
    "test_prediction_lr = regressor_loaded.predict(test_feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f753f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_result : \n",
      "       tweet_id                                               text predictions\n",
      "0    1968834470  Im supposed studying Im eating chips couch wat...    Negative\n",
      "1    1968834487            fun whatever dont get clos mess without    Negative\n",
      "2    1968834543  Why hello fellow Twitizens I going bed I pulle...    Positive\n",
      "3    1968834546                                                new    Positive\n",
      "4    1968834572  I feel naughty happy amp much much muuucccchhh...    Positive\n",
      "..          ...                                                ...         ...\n",
      "995  2326735918                    goodbye long beautiful rad hair    Positive\n",
      "996  2326736140                                        least thats    Positive\n",
      "997  2326736165                                  like songs hayyyy    Positive\n",
      "998  2326736662  INSANE time tonight I wish I could go Ill eat ...    Negative\n",
      "999  2326736730  Sleep rank 1 priority atm I wan na stay I cant...    Negative\n",
      "\n",
      "[1000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Averaging out the hashtags result\n",
    "test_result_ds = pd.DataFrame({'hashtag': test_ds.hashtag,'text':test_ds.text ,'prediction':test_prediction_lr})\n",
    "test_result = test_result_ds.groupby(['hashtag']).max().reset_index()\n",
    "test_result.columns = ['tweet_id','text', 'predictions']\n",
    "test_result.predictions = test_result['predictions'].apply(int_to_string)\n",
    "print(\"test_result : \")\n",
    "print(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b35d18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result_predictions_1000_tweets\n",
      "     Unnamed: 0    tweet_id  \\\n",
      "0             0  1968834470   \n",
      "1             1  1968834487   \n",
      "2             2  1968834543   \n",
      "3             3  1968834546   \n",
      "4             4  1968834572   \n",
      "..          ...         ...   \n",
      "995         995  2326735918   \n",
      "996         996  2326736140   \n",
      "997         997  2326736165   \n",
      "998         998  2326736662   \n",
      "999         999  2326736730   \n",
      "\n",
      "                                                  text predictions  \n",
      "0    Im supposed studying Im eating chips couch wat...    Negative  \n",
      "1              fun whatever dont get clos mess without    Negative  \n",
      "2    Why hello fellow Twitizens I going bed I pulle...    Positive  \n",
      "3                                                  new    Positive  \n",
      "4    I feel naughty happy amp much much muuucccchhh...    Positive  \n",
      "..                                                 ...         ...  \n",
      "995                    goodbye long beautiful rad hair    Positive  \n",
      "996                                        least thats    Positive  \n",
      "997                                  like songs hayyyy    Positive  \n",
      "998  INSANE time tonight I wish I could go Ill eat ...    Negative  \n",
      "999  Sleep rank 1 priority atm I wan na stay I cant...    Negative  \n",
      "\n",
      "[1000 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "#test_result.to_csv('C:/Users/HPr/Desktop/Stage/result_predictions_470_tweets.csv', index=False)\n",
    "test_result.to_csv('C:/Users/HPr/Desktop/2GI/ml/mlml/result_predictions_1000_tweets.csv')\n",
    "\n",
    "df_csv = pd.read_csv('C:/Users/HPr/Desktop/2GI/ml/mlml/result_predictions_1000_tweets.csv')\n",
    "print(\"result_predictions_1000_tweets\")\n",
    "print(df_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375424e4",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1105963a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "380030d1298d5a27518acca789ff38fe82bbf2e68b73263de6a6bf23efb7704c"
  },
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
